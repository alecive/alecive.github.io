---
title: RI Faculty info
permalink: /hidden/fi.html
---

{::options toc_levels="1, 2" /}

# Contents
{:.no_toc}

* This line will be replaced by the ToC, excluding the "Contents" header
{:toc}

# Interesting people I would love to collaborate with

 * [Yaser Ajmal Sheikh](https://www.ri.cmu.edu/ri-faculty/yaser-ajmal-sheikh/) → creator of OpenPose
 * [Yong-lae Park](http://www.cs.cmu.edu/~ylpark/research.html) → soft artificial skin [soft robotics lab](http://softrobotics.cs.cmu.edu/)

# [Illah Nourbakhsh](http://www.cs.cmu.edu/~illah/)

HRI for greater good, a lot of integration work.
**The Community Lab** privileges sustained relationships above and beyond the technology innovation and knowledge-building processes, balancing
technology invention with cultural transformation, and technical literacy with issue-driven advocacy.
To have profound impact, a Community Lab must engage with communities of practice at every stage of ideation, innovation, deployment, evaluation and scaling.

Human-robot interaction with the aim of creating rich, effective and satisfying interactions between humans and robots. My research has focused specifically on human-robot collaboration, wherein the **robotic and human agents in the system share the same unifying goal or utility function**. I further sharpen my scope to **human-robot collaboration for learning**, in which the measurable outcomes are information gain on the part of the humans in the system. In the context of my focus on collaboration for learning, rich means a cognitively sophisticated interaction in which humans and robots communicate as peers; effective means that formal measures of human learning should yield significant outcomes; satisfying means that humans should find the interaction both useful and pleasurable.

 * What enabling competencies in the areas of social perception and control are required for applicability of the resulting models?
 * What principles of robot morphology and robot behavior design have broad applicability to the design of interaction systems?
 * Can a principled interaction evaluation methodology enable us to implement complete, feedback-driven design life cycles for interaction systems?

One research focus has been to develop embedded solutions to the problem of semantic interpretation of events using visual sensing. Another focus has been robot navigation because it is an important prerequisite to many forms of social interaction when the robot shares the human physical space. For example visual-topological navigation and hybrid metric-topological models aim to provide navigation competency with a minimum of computational and memory demands. Because of the cross-disciplinary nature of the human-robot collaboration problem, integrative research must bring robotics together with other fields that model human cognition and social behavior. I have joined and extended models of interaction and evaluation methodology from **Human Factors, HCI and Cognitive Psychology**, outstanding complements to robotics since these fields already consider human relationships to physical embodiments and consider human behavioral change over time.

Most recently we have begun to study the role of a research lab in meaningful design, dissemination and scaling with communities of practice. Our working model is one in which participatory design, design-based thinking and robotic innovation are combined to achieve **positive social impact** on specific problems throughout societies. For specific information about these projects, all of which are dedicated to make real impact while also establishing models for laboratory-community interaction, see my CREATE Lab website.

# [Henny Admoni](http://hennyadmoni.com/)


# [Ralph Hollis](https://www.ri.cmu.edu/ri-faculty/ralph-hollis/)

Robot structures, Kinematics and Dynamics, Haptics, pHRI, Mechanisms and Actuation, Manipulation

* Assembly of small high-precision electromechanical products such as computer storage devices, medical devices, communication devices, and other high-density mechatronic equipment. The goal is to revolutionize the assembly of these kinds of products by drastically reducing the time it takes to design, program, and deploy automated assembly systems, while increasing their precision by several orders of magnitude and reducing their physical size.
* Haptic HCI with computed or remote environments. Here a goal is to enable truly transparent and high-fidelity interaction with eventual application to medicine, computer-augmented design, and telemanipulation, including scaled manipulation of microscopic and nanoscopic objects.
* Intelligent mobile robots which are dynamically stable, including both rolling and walking machines. If such robots are to operate successfully in peopled environments, they must be agile and responsive to physical interaction with humans and their surroundings.

# [Matt Travers](https://www.ri.cmu.edu/ri-faculty/matthew-j-travers/)

Biologically inspired dynamic locomotion and learning, compliant manipulation for agriculture and food preparation, **managing uncertainty in human-robot interaction**, and field-ready search and rescue robotics.

Mechanism design of highly articulated mechanisms for search and rescue robotics

Works in the biorobotics lab that is managed by howie choset

# [Aaron Steinfeld](http://www.cs.cmu.edu/~astein/)

My specialty is human-robot interaction and advanced transportation. I am interested in how to enable timely and appropriate interaction when interfaces are restricted through design, tasks, the environment, time pressures, and/or user abilities. I utilize training and experience in robotics, intelligent transportation, rehabilitation, human factors and ergonomics, human-computer interaction, universal design, and research methods.

From [marynel.net](http://marynel.net): I am a computer scientist and roboticist by training. The goal of my research is to understand how we can build interactive computing technologies that are meaningful, intuitive, and appropriate for users, especially in complex social environments. My research crosses the boundaries between Human-Robot and Human-Computer Interaction, and often combines elements from robot perception, computer vision, machine learning, social science, and design.


# [Martial Hebert](http://www.cs.cmu.edu/~hebert/)

My work is in the areas of computer vision and perception for autonomous systems. My interests are in the interpretation of perception data (both 2-D and 3-D), including building models of environments. Current research directions include:

 * Efficient techniques for object/category recognition
 * Use of contextual information, in particular 3-D geometry from images, for scene analysis
 * Symbolic knowledge for scene interpretation and reconstruction
 * Motion analysis for feature extraction and event detection in video clips
 * Efficient tools for the analysis of dynamic 3-D point clouds (“3-D signal processing”)
 * Perception for autonomous systems
 * Detection, tracking, and prediction in dynamic environments

# [Oliver Kroemer](https://www.ri.cmu.edu/ri-faculty/oliver-kroemer/)

My research focuses on developing algorithms and representations to enable robots to learn versatile manipulation skills over time. The ability to learn skills and adapt manipulations to new situations will open up a wide range of new robot applications, including taking care of the elderly, maintaining parks and public places, and assisting in hazardous environments. I have developed methods for robots to learn about objects through physical interactions and improve their skills autonomously using reinforcement learning. I have also proposed representations for capturing various aspects of manipulations, e.g., contact states and motor primitives, to improve generalization between different scenarios and skills.  The ultimate goal of my research is to develop a life-long learning framework for robots to acquire manipulation skills.

Before joining the CMU Robotics Institute in 2018, I was a postdoctoral researcher at the University of Southern California (USC). I received my Masters and Bachelors degrees in engineering from the University of Cambridge in 2008.  From 2009 to 2011, I  was a Ph.D. student at the Max Planck Institute for Intelligent Systems. In 2014, I defended my Ph.D. thesis at the Technische Universitaet Darmstadt and was a finalist for the 2015 Georges Giralt Ph.D. Award for the best robotics Ph.D. thesis in Europe.

# [Katharina Muelling](https://sites.google.com/site/katharinamuelling/)

My work is driven by the passion of getting robots out of research laboratories into the real world. To this end, I seek to understand how complex motor behavior can be modeled and learned, how motor skills can be generalized, how humans and robots can work together to achieve complex tasks, and how we can use these technologies to improve the quality of life for people with disabilities.

Robotic systems that are able to perform various tasks in human-inhabited and unstructured environ- ments require robust movement generation and manipulation skills that compensate for uncertainties and disturbances in the environment. Such systems need to autonomously adapt to a highly dynamic environ- ment while simultaneously accomplishing the task at hand. Hand-crafted solutions based on analytical engineering often fail to produce this form of adaptive behavior and are limited to the specific scenarios considered by the designer. Here, machine learning is a promising tool to create robotic systems that are able to adapt to a dynamic, unstructured environment. Most generic machine learning approaches how- ever, fail to learn on real robot platforms due to specific constraints and restrictions such as the real-time requirements and hardware exhaustion.

My work focuses on finding the limits of analytical engineering solutions and the development of robot learning algorithms that circumvent these limits. A fundamental problem for the development of robot learning methods is the necessity to achieve complex behaviors with a feasible amount of training data. Human demonstrations can be used to initialize robot learning approaches and reduce the learning time significantly. Furthermore, it provides a natural way for humans to teach robots and allows robots to acquire human-like behavior which is beneficial for human-robot interaction. Therefore, learning from and with humans is a central part of my work.

In the following, I will provide an overview of my research to date, together with research directions which I intend to pursue as a system faculty member at Carnegie Mellon University. The presented work is centered around (i) my doctoral studies, which focused on the development of a table tennis framework for an anthropomorphic robot arm and (ii) my current research that assists a tetraplegic person in teleoperating a robot arm to successfully perform everyday manipulation tasks such as opening a door.

Research Areas
Autonomy Infused Teleoperation

Teleoperating a robot arm to perform fine manipulation and every day living tasks can be hard due to challenges including latency, intermittency, and asymmetry in control inputs. These problems are particularly relevant when the user is physically impaired and has to control the arm through input devices such as brain computer interfaces, EMG, or 2D joysticks. The combination of autonomous robot technologies and direct user control in  a shared-control teleoperation framework can help to overcome the involved challenges. Our autonomy infused teleoperation architecture combines computer vision, user-intent inference and human robot arbitration  in order to produce supervised autonomous manipulation. The goal driving this work is to produce intuitive control that enables human-like manipulation with noisy and sometimes erratic input signals.
Motor Skill Learning

Robotic systems that are able to perform various tasks in human-inhabited and unstructured environments require robust movement generation and manipulation skills that compensate for uncertainties and disturbances in the environment. Such systems need to autonomously adapt to a highly dynamic environment while simultaneously accomplishing the task at hand. I am interested in developing machine learning algorithms for learning motor skills that can circumvent the limits of analytical engineered solutions. A fundamental problem for the development of robot learning methods is the necessity to achieve complex behaviors with a feasible amount of training data. Human demonstrations can be used to initialize robot learning approaches and reduce the learning time significantly. Furthermore, it provides a natural way for humans to teach robots and allows robots to acquire human-like behavior which is beneficial for human-robot interaction.


Learning Higher-Level Behavior

A motor behavior is always directed towards achieving a specific goal. In
table tennis, this goal is winning the game. But what is the best way to achieve this goal? While symbolic planning has been successfully applied in many classical AI areas, they fail to scale to real robot behaviors especially if human interactions are involved. This is due to their limitations to model the uncertainty of actions, to address geometric and kinematic constraints and to model human behavior. I am interested in modeling and learning such higher-level decision processes to enable efficient and human-like robot behavior and problem solving.
Social and Interactive Motion Planning

Creating autonomous and intelligent systems that are able to move out of the factory floors into human inhabited environments bears many challenges. While the state-of-the art in robot motion planning has made tremendous progress in planning in high-dimensional and even dynamic environments, it is still hard for robots to navigate through a crowded environment and to interact with humans in a safe and socially acceptable manner. To enable the robot systems to work with and close by humans they (i) need to be able to infer the intent of the human and to integrate it in an efficient manner into the planning process, (ii) behave in an human understandable manner, (iii) interact with the human in a social manner.

# [Maxim Likhachev](http://www.cs.cmu.edu/~maxim/)

I am a Research Associate Professor with the Robotics Institute and National Robotics Engineering Center (NREC), both part of School of Computer Science, Carnegie Mellon University. I am also an adjunct faculty at the Computer and Information Science department at University of Pennsylvania and a member of the GRASP laboratory. Finally, I'm also a co-founder of TravelWits.com, a site that uses techniques from Artificial Intelligence to find cheaper ways to travel.

Research: My general research interests lie in Artificial Intelligence and Robotics. More specifically, my group studies such questions as high-dimensional planning in real-time, planning that "learns how to plan" based on experience, automatic generation of compact representation of planning problems, high-dimensional planning under uncertainty in real-time and others. Most of our approaches are centered around the development of new graph search algorithms for real-time planning on huge graphs, study of new graph search algorithms for learning how to search these graphs efficiently from experience and methods for learning compact graph representations of planning problems.

My group strives to develop methods with rigorous theoretical guarantees and use these methods to build planners for physical robots performing challenging tasks in real-world environments. Some of the robotic systems my group does planning for include mobile manipulators, aerial vehicles, multi-robot systems, and humanoid-like robots (see some of the robots in my lab below). I do get easily motivated, however, by other interesting planning problems.

During my 2-year Postdoctoral Fellowship at CMU, I worked with Tony Stentz on multi-agent planning with adversaries. During the same time, I have also worked on the design and implementation of a complex maneuvers planner for the CMU vehicle that won the Urban Challenge race (the third DARPA Grand Challenge). During my Ph.D. studies at CMU, my advisors were Geoff Gordon and Sebastian Thrun (presently at Stanford). Before enrolling into the Ph.D. program at CMU I have been a graduate student for two years in the College of Computing at Georgia Tech where I worked with Ronald Arkin at Mobile Robot Laboratory and Sven Koenig.

# [Jean Oh](http://www.cs.cmu.edu/~jeanoh/)

My research interests lie in the area of AI in robotics including autonomy, language understanding, multimodal perception, path planning, and machine learning. I am interested in creating persistent robots that can co-exist with humans in shared environments, learning to improve themselves over time through continuous training, exploration, and interactions. Toward this general goal, I am currently focusing on the translation of information among vision, language, and planning so that robots can 1) perceive better by fusing with verbal information provided by humans, 2) describe what they have observed in natural language, 3) generate semantic navigation plans by following complex directions, 4) learn to navigate in a socially compliant way, and 5) explain their past, current, and future actions and plans. As for the communication modalities for robots, I am primarily interested in verbal (text or speech) communication in natural language, but I am also interested in alternative ways for robots to express what goes on in their internal states, e.g., communication modalities studied in the arts and design community such as media arts.

Currently, I am leading the perception and the learning tasks for the DARPA Aircrew-Labor In-Cockpit Automation System (ALIAS) program (co-PI) that aims to bring intelligence to cockpits through semantic perception and learning new skills from observing experienced pilots. I am also a PI on the intelligence architecture subtask of the ARL RCTA program where my team’s work on language understanding on robot navigation won the Best Cognitive Robotics Paper Award at the IEEE International Conference on Robotics and Automation (ICRA) in 2015. My newest project is the US DoD – Korea MOTIE (co-PI) collaboration on robotics technologies for disaster response, where I will be leading the semantic map construction by leveraging text data from social media and crowdsourcing.


# [Hartmut Geyer](https://www.cs.cmu.edu/~hgeyer/Research_Main.html)

My background is in physics.  The workings of nature have always fascinated me.  But I am most intrigued by how life exploits the laws of physics to solve tough engineering problems.

Dynamics and control of legged locomotion is such a tough engineering problem.  The legged robots that we have built so far struggle with it;  they barely survive outside the laboratory environment.  Nor do artificial legs reach far beyond peg legs that offer a mere support to their users.  Even when legs are perfectly intact, as in spinal cord injured patients, we do not know how to engineer locomotion controls for them.

Animals and humans, by contrast, display an unsurpassed agility and ease at mastering the problem of legged dynamics and control.

In my research, I try to understand

 * the principles of legged dynamic systems underlying animal and human locomotion,
 * how these principles impact on human motor control,

and to apply this understanding to

 * the design and control of legs in humanoid and rehabilitation robotics.

Fig 1: From Principles to Motor Control to Rehabitation
Figure: Research Interests

I believe that understanding the principles of legged dynamics and their connection to motor control will change how we simulate human locomotion, engineer artificial legs, and rehabilitate patients.

This research involves developing mathematical and computational models that capture essential problems of locomotion biomechanics and motor control, conducting animal and human experiments that test and inspire these models, and building robot legs that translate the model results into rehabilitation technologies.

Follow the links above or on the left for more details about my research.

# [Chris Atkeson](http://www.cs.cmu.edu/~cga/)

My life goal is to fulfill the science fiction vision of machines that achieve human levels of competence in perceiving, thinking, and acting. A more narrow technical goal is to understand how to get machines to generate and perceive human behavior. I use two complementary approaches, exploring humanoid robotics and human aware environments. Building humanoid robots tests our understanding of how to generate human-like behavior, and exposes the gaps and failures in current approaches. Building human aware environments (environments that perceive human activity and estimate human internal state) pushes the development of machine perception of humans. In addition to being socially useful, building human aware environments helps us develop humanoid robots that are capable of understanding and interacting with humans.

Machine learning underlies much of my work in both humanoid robotics and human aware environments. I am an experimentalist in the field of robot learning, specializing in the learning of challenging dynamic tasks such as juggling. I combine designing learning algorithms with exploring their behavior in implementations on actual robots and in intelligent environments. My research interests include nonparametric learning, memory-based learning, reinforcement learning, learning from demonstration, and modeling human behavior.

Perception: Robot Skin

I have a longstanding interest in perception and aware environments (Classroom 2000, Aware Home, CareMedia, see the biographical slides mentioned above for more information). Now I am applying what I have learned from building those systems to robot skin. The key idea is to cover the robot with eyeballs (cameras), and have transparent skin. The current prototype (FingerVision) is described in "Implementing Tactile Behaviors Using FingerVision", A. Yamaguchi and C. G. Atkeson, Humanoids 2017. More papers and videos on FingerVision, slides, and an NSF proposal.

# [David Held](http://davheld.github.io/)

Young faculty

My research lies at the intersection of robotics, machine learning, and computer vision.

I am interested in developing methods for robotic perception and control that can allow robots to operate in in the messy, cluttered environments of our daily lives. My approach is to design new deep learning / machine learning algorithms to understand environmental changes: how dynamic objects in the environment can move and how to affect the environment to achieve a desired task.

I have applied this idea of learning to understand environmental changes to improve a robot's capabilities in two domains: object manipulation and autonomous driving. I am currently working on learning to control indoor robots for various object manipulation tasks, dealing with questions about multi-task learning, robust learning, simulation to real-world transfer, and safety. Within autonomous driving, I have shown how, by modeling object appearance changes, we can improve a robot's capabilities for every part of the robot perception pipeline: segmentation, tracking, velocity estimation, and object recognition. By teaching robots to understand and affect environmental changes, I hope to open the door to many new robotics applications, such as robots for our homes, assisted living facilities, schools, hospitals, or disaster relief areas.
The following video provides a decent overview of my current research and some of my current interests:
