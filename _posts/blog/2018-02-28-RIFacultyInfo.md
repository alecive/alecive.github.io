---
title: RI Faculty info
permalink: /hidden/fi.html
---

{::options toc_levels="1, 2" /}

# Contents
{:.no_toc}

* This line will be replaced by the ToC, excluding the "Contents" header
{:toc}

# Interesting people I would love to collaborate with

 * [Yaser Ajmal Sheikh](https://www.ri.cmu.edu/ri-faculty/yaser-ajmal-sheikh/) → creator of OpenPose
 * [Yong-lae Park](http://www.cs.cmu.edu/~ylpark/research.html) → soft artificial skin [soft robotics lab](http://softrobotics.cs.cmu.edu/)

# [Illah Nourbakhsh](http://www.cs.cmu.edu/~illah/)

HRI for greater good, a lot of integration work.
**The Community Lab** privileges sustained relationships above and beyond the technology innovation and knowledge-building processes, balancing
technology invention with cultural transformation, and technical literacy with issue-driven advocacy.
To have profound impact, a Community Lab must engage with communities of practice at every stage of ideation, innovation, deployment, evaluation and scaling.

Human-robot interaction with the aim of creating rich, effective and satisfying interactions between humans and robots. My research has focused specifically on human-robot collaboration, wherein the **robotic and human agents in the system share the same unifying goal or utility function**. I further sharpen my scope to **human-robot collaboration for learning**, in which the measurable outcomes are information gain on the part of the humans in the system. In the context of my focus on collaboration for learning, rich means a cognitively sophisticated interaction in which humans and robots communicate as peers; effective means that formal measures of human learning should yield significant outcomes; satisfying means that humans should find the interaction both useful and pleasurable.

 * What enabling competencies in the areas of social perception and control are required for applicability of the resulting models?
 * What principles of robot morphology and robot behavior design have broad applicability to the design of interaction systems?
 * Can a principled interaction evaluation methodology enable us to implement complete, feedback-driven design life cycles for interaction systems?

I have joined and extended models of interaction and evaluation methodology from **Human Factors, HCI and Cognitive Psychology**, outstanding complements to robotics since these fields already consider human relationships to physical embodiments and consider human behavioral change over time.

Our working model is one in which participatory design, design-based thinking and robotic innovation are combined to achieve **positive social impact** on specific problems throughout societies. For specific information about these projects, all of which are dedicated to make real impact while also establishing models for laboratory-community interaction, see my CREATE Lab website.

# [Henny Admoni](http://hennyadmoni.com/)


# [Ralph Hollis](https://www.ri.cmu.edu/ri-faculty/ralph-hollis/)

Robot structures, Kinematics and Dynamics, Haptics, pHRI, Mechanisms and Actuation, Manipulation

* Assembly of small high-precision electromechanical products such as computer storage devices, medical devices, communication devices, and other high-density mechatronic equipment. The goal is to revolutionize the assembly of these kinds of products by drastically reducing the time it takes to design, program, and deploy automated assembly systems, while increasing their precision by several orders of magnitude and reducing their physical size.
* Haptic HCI with computed or remote environments. Here a goal is to enable truly transparent and high-fidelity interaction with eventual application to medicine, computer-augmented design, and telemanipulation, including scaled manipulation of microscopic and nanoscopic objects.
* Intelligent mobile robots which are dynamically stable, including both rolling and walking machines. If such robots are to operate successfully in peopled environments, they must be agile and responsive to physical interaction with humans and their surroundings.

# [Matt Travers](https://www.ri.cmu.edu/ri-faculty/matthew-j-travers/)

Works in the biorobotics lab that is managed by howie choset

Biologically inspired dynamic locomotion and learning, compliant manipulation for agriculture and food preparation, **managing uncertainty in human-robot interaction**, and field-ready search and rescue robotics.

Mechanism design of highly articulated mechanisms for search and rescue robotics

# [Aaron Steinfeld](http://www.cs.cmu.edu/~astein/)

My specialty is **human-robot interaction** and advanced transportation. I am interested in how to enable timely and appropriate interaction when interfaces are restricted through design, tasks, the environment, time pressures, and/or user abilities. I utilize training and experience in robotics, intelligent transportation, rehabilitation, human factors and ergonomics, human-computer interaction, universal design, and research methods.

From [marynel.net](http://marynel.net): The goal of my research is to understand how we can build interactive computing technologies that are meaningful, intuitive, and appropriate for users, especially in complex social environments. Elements from robot perception, computer vision, machine learning, social science, and design.

# [Martial Hebert](http://www.cs.cmu.edu/~hebert/)

My work is in the areas of computer vision and perception for autonomous systems. My interests are in the interpretation of perception data (both 2-D and 3-D), including building models of environments. Current research directions include:

 * Efficient techniques for object/category recognition
 * Use of contextual information, in particular 3-D geometry from images, for scene analysis
 * Symbolic knowledge for scene interpretation and reconstruction
 * Motion analysis for feature extraction and event detection in video clips
 * Efficient tools for the analysis of dynamic 3-D point clouds (“3-D signal processing”)
 * Perception for autonomous systems
 * Detection, tracking, and prediction in dynamic environments

# [Oliver Kroemer](https://www.ri.cmu.edu/ri-faculty/oliver-kroemer/)

**New Faculty**

Representations to enable robots to learn versatile manipulation skills over time. Methods for robots to **learn about objects through physical interactions** and improve their skills autonomously using **reinforcement learning**. I have also proposed representations for capturing various aspects of manipulations, e.g., contact states and motor primitives, to improve generalization between different scenarios and skills. The ultimate goal of my research is to develop a **life-long learning framework for robots to acquire manipulation skills.**

# [Katharina Muelling](https://sites.google.com/site/katharinamuelling/)

How complex motor behavior can be modeled and learned, how motor skills can be generalized, how humans and robots can work together to achieve complex tasks, and how we can use these technologies to improve the quality of life for people with disabilities.

 * Autonomy Infused Teleoperation → Teleoperating a robot arm to perform fine manipulation and every day living tasks can be hard due to challenges including latency, intermittency, and asymmetry in control inputs.
 * Motor Skill Learning → Machine learning algorithms for learning motor skills
 * Learning Higher-Level Behavior → While symbolic planning has been successfully applied in many classical AI areas, they fail to scale to real robot behaviors especially if human interactions are involved. This is due to their limitations to model the uncertainty of actions, to address geometric and kinematic constraints and to model human behavior.
 * Social and Interactive Motion Planning → While the state-of-the art in robot motion planning has made tremendous progress in planning in high-dimensional and even dynamic environments, it is still hard for robots to navigate through a crowded environment and to interact with humans in a safe and socially acceptable manner. To enable the robot systems to work with and close by humans they (i) need to be able to infer the intent of the human and to integrate it in an efficient manner into the planning process, (ii) behave in an human understandable manner, (iii) interact with the human in a social manner.

# [Maxim Likhachev](http://www.cs.cmu.edu/~maxim/)

High-dimensional planning in real-time, **planning that "learns how to plan"** based on experience, automatic generation of compact representation of planning problems, high-dimensional planning under uncertainty in real-time and others. Most of our approaches are centered around the development of new graph search algorithms for real-time planning on huge graphs, study of new graph search algorithms for learning how to search these graphs efficiently from experience and methods for learning compact graph representations of planning problems.

# [Jean Oh](http://www.cs.cmu.edu/~jeanoh/)

My research interests lie in the area of AI in robotics including autonomy, language understanding, multimodal perception, path planning, and machine learning. Robots that can

 * perceive better by fusing with verbal information provided by humans
 * describe what they have observed in natural language
 * generate semantic navigation plans by following complex directions
 * learn to navigate in a socially compliant way
 * explain their past, current, and future actions and plans

As for the communication modalities for robots, I am primarily interested in verbal (text or speech) communication in natural language, but I am also interested in alternative ways for robots to express what goes on in their internal states, e.g., communication modalities studied in the arts and design community such as media arts.

Currently, I am leading the perception and the learning tasks for the DARPA Aircrew-Labor In-Cockpit Automation System (ALIAS) program (co-PI) that aims to bring intelligence to cockpits through semantic perception and learning new skills from observing experienced pilots. I am also a PI on the intelligence architecture subtask of the ARL RCTA program where my team’s work on language understanding on robot navigation won the Best Cognitive Robotics Paper Award at the IEEE International Conference on Robotics and Automation (ICRA) in 2015. My newest project is the US DoD – Korea MOTIE (co-PI) collaboration on robotics technologies for disaster response, where I will be leading the semantic map construction by leveraging text data from social media and crowdsourcing.

# [Hartmut Geyer](https://www.cs.cmu.edu/~hgeyer/Research_Main.html)

My background is in **physics**. The workings of nature have always fascinated me. But I am most intrigued by how life exploits the laws of physics to solve tough engineering problems.

Dynamics and control of legged locomotion is such a tough engineering problem. The legged robots that we have built so far struggle with it; they barely survive outside the laboratory environment.  Nor do artificial legs reach far beyond peg legs that offer a mere support to their users. Even when legs are perfectly intact, as in spinal cord injured patients, we do not know how to engineer locomotion controls for them.

In my research, I try to understand the principles of legged dynamic systems underlying animal and human locomotion, how these principles impact on human motor control, and to apply this understanding to the design and control of legs in humanoid and rehabilitation robotics.

I believe that understanding the principles of legged dynamics and their connection to motor control will change how we simulate human locomotion, engineer artificial legs, and rehabilitate patients.

# [Chris Atkeson](http://www.cs.cmu.edu/~cga/)

My life goal is to fulfill the science fiction vision of machines that achieve human levels of competence in perceiving, thinking, and acting. A more narrow technical goal is to understand how to get machines to generate and perceive human behavior. I use two complementary approaches, exploring humanoid robotics and human aware environments. Building humanoid robots tests our understanding of how to generate human-like behavior, and exposes the gaps and failures in current approaches. Building human aware environments (environments that perceive human activity and estimate human internal state) pushes the development of machine perception of humans. In addition to being socially useful, building human aware environments helps us develop humanoid robots that are capable of understanding and interacting with humans.

Machine learning underlies much of my work in both humanoid robotics and human aware environments. I am an experimentalist in the field of robot learning, specializing in the learning of challenging dynamic tasks such as juggling. I combine designing learning algorithms with exploring their behavior in implementations on actual robots and in intelligent environments. My research interests include nonparametric learning, memory-based learning, reinforcement learning, learning from demonstration, and modeling human behavior.

Perception: Robot Skin

I have a longstanding interest in perception and aware environments (Classroom 2000, Aware Home, CareMedia, see the biographical slides mentioned above for more information). Now I am applying what I have learned from building those systems to robot skin. The key idea is to cover the robot with eyeballs (cameras), and have transparent skin. [FingerVision, Humanoids 2017].

# [David Held](http://davheld.github.io/)

Young faculty

My research lies at the intersection of robotics, machine learning, and computer vision.

I am interested in developing methods for robotic perception and control that can allow robots to operate in in the messy, cluttered environments of our daily lives. My approach is to design new deep learning / machine learning algorithms to understand environmental changes: how dynamic objects in the environment can move and how to affect the environment to achieve a desired task.

I have applied this idea of learning to understand environmental changes to improve a robot's capabilities in two domains: **object manipulation** and autonomous driving. I am currently working on learning to control indoor robots for various object manipulation tasks, dealing with questions about multi-task learning, robust learning, simulation to real-world transfer, and safety. Within autonomous driving, I have shown how, by modeling object appearance changes, we can improve a robot's capabilities for every part of the robot perception pipeline: segmentation, tracking, velocity estimation, and object recognition. By teaching robots to understand and affect environmental changes, I hope to open the door to many new robotics applications, such as robots for our homes, assisted living facilities, schools, hospitals, or disaster relief areas.
The following video provides a decent overview of my current research and some of my current interests:
