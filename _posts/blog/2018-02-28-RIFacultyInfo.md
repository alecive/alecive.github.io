---
title: RI Faculty info
permalink: /hidden/fi.html
---

{::options toc_levels="1, 2" /}

# Contents
{:.no_toc}

* This line will be replaced by the ToC, excluding the "Contents" header
{:toc}

# Interesting people I would love to collaborate with

 * [Yaser Ajmal Sheikh](https://www.ri.cmu.edu/ri-faculty/yaser-ajmal-sheikh/) → creator of OpenPose
 * [Yong-lae Park](http://www.cs.cmu.edu/~ylpark/research.html) → soft artificial skin [soft robotics lab](http://softrobotics.cs.cmu.edu/)

# [Illah Nourbakhsh](http://www.cs.cmu.edu/~illah/)

HRI for greater good, a lot of integration work.
**The Community Lab** privileges sustained relationships above and beyond the technology innovation and knowledge-building processes, balancing
technology invention with cultural transformation, and technical literacy with issue-driven advocacy.
To have profound impact, a Community Lab must engage with communities of practice at every stage of ideation, innovation, deployment, evaluation and scaling.

Human-robot interaction with the aim of creating rich, effective and satisfying interactions between humans and robots. My research has focused specifically on human-robot collaboration, wherein the **robotic and human agents in the system share the same unifying goal or utility function**. I further sharpen my scope to **human-robot collaboration for learning**, in which the measurable outcomes are information gain on the part of the humans in the system. In the context of my focus on collaboration for learning, rich means a cognitively sophisticated interaction in which humans and robots communicate as peers; effective means that formal measures of human learning should yield significant outcomes; satisfying means that humans should find the interaction both useful and pleasurable.

 * What enabling competencies in the areas of social perception and control are required for applicability of the resulting models?
 * What principles of robot morphology and robot behavior design have broad applicability to the design of interaction systems?
 * Can a principled interaction evaluation methodology enable us to implement complete, feedback-driven design life cycles for interaction systems?

I have joined and extended models of interaction and evaluation methodology from **Human Factors, HCI and Cognitive Psychology**, outstanding complements to robotics since these fields already consider human relationships to physical embodiments and consider human behavioral change over time.

Our working model is one in which participatory design, design-based thinking and robotic innovation are combined to achieve **positive social impact** on specific problems throughout societies. For specific information about these projects, all of which are dedicated to make real impact while also establishing models for laboratory-community interaction, see my CREATE Lab website.

# [Henny Admoni](http://hennyadmoni.com/)


# [Ralph Hollis](https://www.ri.cmu.edu/ri-faculty/ralph-hollis/)

Robot structures, Kinematics and Dynamics, Haptics, pHRI, Mechanisms and Actuation, Manipulation

* Assembly of small high-precision electromechanical products such as computer storage devices, medical devices, communication devices, and other high-density mechatronic equipment. The goal is to revolutionize the assembly of these kinds of products by drastically reducing the time it takes to design, program, and deploy automated assembly systems, while increasing their precision by several orders of magnitude and reducing their physical size.
* Haptic HCI with computed or remote environments. Here a goal is to enable truly transparent and high-fidelity interaction with eventual application to medicine, computer-augmented design, and telemanipulation, including scaled manipulation of microscopic and nanoscopic objects.
* Intelligent mobile robots which are dynamically stable, including both rolling and walking machines. If such robots are to operate successfully in peopled environments, they must be agile and responsive to physical interaction with humans and their surroundings.

# [Aaron Steinfeld](http://www.cs.cmu.edu/~astein/)

My specialty is **human-robot interaction** and advanced transportation. I am interested in how to enable timely and appropriate interaction when interfaces are restricted through design, tasks, the environment, time pressures, and/or user abilities. I utilize training and experience in robotics, intelligent transportation, rehabilitation, human factors and ergonomics, human-computer interaction, universal design, and research methods.

From [marynel.net](http://marynel.net): The goal of my research is to understand how we can build interactive computing technologies that are meaningful, intuitive, and appropriate for users, especially in complex social environments. Elements from robot perception, computer vision, machine learning, social science, and design.


# [Chris Atkeson](http://www.cs.cmu.edu/~cga/)

My life goal is to fulfill the science fiction vision of machines that achieve human levels of competence in perceiving, thinking, and acting. A more narrow technical goal is to understand how to get machines to generate and perceive human behavior. I use two complementary approaches, exploring humanoid robotics and human aware environments. Building humanoid robots tests our understanding of how to generate human-like behavior, and exposes the gaps and failures in current approaches. Building human aware environments (environments that perceive human activity and estimate human internal state) pushes the development of machine perception of humans. In addition to being socially useful, building human aware environments helps us develop humanoid robots that are capable of understanding and interacting with humans.

Machine learning underlies much of my work in both humanoid robotics and human aware environments. I am an experimentalist in the field of robot learning, specializing in the learning of challenging dynamic tasks such as juggling. I combine designing learning algorithms with exploring their behavior in implementations on actual robots and in intelligent environments. My research interests include nonparametric learning, memory-based learning, reinforcement learning, learning from demonstration, and modeling human behavior.

Perception: Robot Skin

I have a longstanding interest in perception and aware environments (Classroom 2000, Aware Home, CareMedia, see the biographical slides mentioned above for more information). Now I am applying what I have learned from building those systems to robot skin. The key idea is to cover the robot with eyeballs (cameras), and have transparent skin. [FingerVision, Humanoids 2017].

# [Katya Sycara](http://www.cs.cmu.edu/~sycara/)

The issue of learning and adaptation in multi-agent systems has been given increasing attention in artificial intelligence research. It is becoming clear, given the dynamic environments in which we want our agent teams to interact, that behavioral repertoires and activities cannot simply be defined in advance. Our approach to multi-agent learning, unlike the top-down model of assuming an agent's state in advance, is notable for its similarity to the types of learning exhibited by lower animal societies.

Research Goal

Our research goal is to enable multiple agents to learn a cooperative and coordinated behavior in a dynamic environment using reinforcement learning.

Assumptions

    An agent does not have any prior knowledge.

    An agent is a self-interested entity and behaves to achieve maximum reward in the range of its knowledge of the environment.

Method

Our method is Profit Sharing Plan (PSP), which is a type of reinforcement learning algorithm. The PSP algorithm allows an autonomous agent to learn a behavior progressively without any instruction and only with delayed rewards. PSP differs from other approaches to learning (like Markov Decision Processes) in that it does not assume an agent's state in advance.



In artificial intelligence research, agent-based systems technology has been hailed as a new paradigm for conceptualizing, designing, and implementing software systems. Agents are sophisticated computer programs that act autonomously on behalf of their users, across open and distributed environments, to solve a growing number of complex problems. Increasingly, however, applications require multiple agents that can work together. A multi-agent system (MAS) is a loosely coupled network of software agents that interact to solve problems that are beyond the individual capacities or knowledge of each problem solver.

Advantages of a Multi-Agent Approach
An MAS has the following advantages over a single agent or centralized approach:

    An MAS distributes computational resources and capabilities across a network of interconnected agents. Whereas a centralized system may be plagued by resource limitations, performance bottlenecks, or critical failures, an MAS is decentralized and thus does not suffer from the "single point of failure" problem associated with centralized systems.

    An MAS allows for the interconnection and interoperation of multiple existing legacy systems. By building an agent wrapper around such systems, they can be inporporated into an agent society.

    An MAS models problems in terms of autonomous interacting component-agents, which is proving to be a more natural way of representing task allocation, team planning, user preferences, open environments, and so on.

    An MAS efficiently retrieves, filters, and globally coordinates information from sources that are spatially distributed.

    An MAS provides solutions in situations where expertise is spatially and temporally distributed.

    An MAS enhances overall system performance, specifically along the dimensions of computational efficiency, reliability, extensibility, robustness, maintainability, responsiveness, flexibility, and reuse.

# [Scott Hudson](https://www.hcii.cmu.edu/people/scott-hudson)

 Scott Hudson is a Professor in the Human-Computer Interaction Institute within the School of Computer Science at Carnegie Mellon University where he serves as the founding director of the HCII PhD program. He was previously an Associate Professor in the College of Computing at the Georgia Institute of Technology and prior to that an Assistant Professor of Computer Science at the University of Arizona. He earned his Ph.D. in Computer Science at the University of Colorado in 1986.

Elected to the CHI Academy in 2006, he has published over 150 technical papers. He has regularly served on program committees for the SIGCHI and UIST conferences, served as papers co-chair for CHI '09, and again for CHI '10. He has previously served as Program Chair for UIST '90 and UIST '00, as well as Symposium Chair for UIST '93 and the founding UIST Doctoral Symposium chair from 2003 to 2005. He also served as a founding Associate Editor for ACM Transactions on Computer Human Interaction. His recent research funding has been from the National Science Foundation, Microsoft, and Disney Research.

You can find out more about Scott's research here and teaching here. The best way to contact Scott is by email at  scott.hudson (at) cs.cmu.edu  (or see the contact information page).

My research interests have covered a wide range of topics within the area of user interface software and technology. However, my work has always revolved around the invention and building of things which lead to a better user experience (although often indirectly through tools for the UI developer). This page provides a brief overview of some past and current projects. This list will inevitably be incomplete and out of date -- see my CV or papers in the ACM Digital Library for a more complete picture.

# [Matt Travers](https://www.ri.cmu.edu/ri-faculty/matthew-j-travers/)

Works in the biorobotics lab that is managed by howie choset

Biologically inspired dynamic locomotion and learning, compliant manipulation for agriculture and food preparation, **managing uncertainty in human-robot interaction**, and field-ready search and rescue robotics.

Mechanism design of highly articulated mechanisms for search and rescue robotics

# [Martial Hebert](http://www.cs.cmu.edu/~hebert/)

My work is in the areas of computer vision and perception for autonomous systems. My interests are in the interpretation of perception data (both 2-D and 3-D), including building models of environments. Current research directions include:

 * Efficient techniques for object/category recognition
 * Use of contextual information, in particular 3-D geometry from images, for scene analysis
 * Symbolic knowledge for scene interpretation and reconstruction
 * Motion analysis for feature extraction and event detection in video clips
 * Efficient tools for the analysis of dynamic 3-D point clouds (“3-D signal processing”)
 * Perception for autonomous systems
 * Detection, tracking, and prediction in dynamic environments

# [Oliver Kroemer](https://www.ri.cmu.edu/ri-faculty/oliver-kroemer/)

**New Faculty**

Representations to enable robots to learn versatile manipulation skills over time. Methods for robots to **learn about objects through physical interactions** and improve their skills autonomously using **reinforcement learning**. I have also proposed representations for capturing various aspects of manipulations, e.g., contact states and motor primitives, to improve generalization between different scenarios and skills. The ultimate goal of my research is to develop a **life-long learning framework for robots to acquire manipulation skills.**

# [Katharina Muelling](https://sites.google.com/site/katharinamuelling/)

How complex motor behavior can be modeled and learned, how motor skills can be generalized, how humans and robots can work together to achieve complex tasks, and how we can use these technologies to improve the quality of life for people with disabilities.

 * Autonomy Infused Teleoperation → Teleoperating a robot arm to perform fine manipulation and every day living tasks can be hard due to challenges including latency, intermittency, and asymmetry in control inputs.
 * Motor Skill Learning → Machine learning algorithms for learning motor skills
 * Learning Higher-Level Behavior → While symbolic planning has been successfully applied in many classical AI areas, they fail to scale to real robot behaviors especially if human interactions are involved. This is due to their limitations to model the uncertainty of actions, to address geometric and kinematic constraints and to model human behavior.
 * Social and Interactive Motion Planning → While the state-of-the art in robot motion planning has made tremendous progress in planning in high-dimensional and even dynamic environments, it is still hard for robots to navigate through a crowded environment and to interact with humans in a safe and socially acceptable manner. To enable the robot systems to work with and close by humans they (i) need to be able to infer the intent of the human and to integrate it in an efficient manner into the planning process, (ii) behave in an human understandable manner, (iii) interact with the human in a social manner.

# [Maxim Likhachev](http://www.cs.cmu.edu/~maxim/)

High-dimensional planning in real-time, **planning that "learns how to plan"** based on experience, automatic generation of compact representation of planning problems, high-dimensional planning under uncertainty in real-time and others. Most of our approaches are centered around the development of new graph search algorithms for real-time planning on huge graphs, study of new graph search algorithms for learning how to search these graphs efficiently from experience and methods for learning compact graph representations of planning problems.

# [Hanbyul Joo](http://www.cs.cmu.edu/~hanbyulj/)

 I am a Ph.D. candidate (expect to graduate in May 2018) in the Robotics Institute at Carnegie Mellon University, under the supervision of Prof. Yaser Sheikh. I am also actively collaborating with Prof. Mina Cikara in Harvard University and Prof. Hyun Soo Park in UMN. I interned at Oculus Research Pittsburgh (Summer and Fall, 2017) and Disney Research Zurich (Summer, 2015). Before joining CMU, I spent three years as a researcher at ETRI, a government-funded research organization in Korea. I received my M.S. in Electrical Engineering (under the supervision of Prof. In So Kweon), and B.S. in Computer Science, both from KAIST, Korea. The Samsung Scholarship is supporting my graduate study.
Research

The goal of my research is the creation of a new scientific discipline of computational behavioral science, by measuring the full spectrum of social signals transmitted during an interpersonal social interaction. I have been developing a unique sensing system at CMU, the Panoptic Studio, which is composed of more than 500 synchronized cameras. My research is based on the tools of computer vision, machine learning, computer graphics, and robotics.




# [Jean Oh](http://www.cs.cmu.edu/~jeanoh/)

My research interests lie in the area of AI in robotics including autonomy, language understanding, multimodal perception, path planning, and machine learning. Robots that can

 * perceive better by fusing with verbal information provided by humans
 * describe what they have observed in natural language
 * generate semantic navigation plans by following complex directions
 * learn to navigate in a socially compliant way
 * explain their past, current, and future actions and plans

As for the communication modalities for robots, I am primarily interested in verbal (text or speech) communication in natural language, but I am also interested in alternative ways for robots to express what goes on in their internal states, e.g., communication modalities studied in the arts and design community such as media arts.

Currently, I am leading the perception and the learning tasks for the DARPA Aircrew-Labor In-Cockpit Automation System (ALIAS) program (co-PI) that aims to bring intelligence to cockpits through semantic perception and learning new skills from observing experienced pilots. I am also a PI on the intelligence architecture subtask of the ARL RCTA program where my team’s work on language understanding on robot navigation won the Best Cognitive Robotics Paper Award at the IEEE International Conference on Robotics and Automation (ICRA) in 2015. My newest project is the US DoD – Korea MOTIE (co-PI) collaboration on robotics technologies for disaster response, where I will be leading the semantic map construction by leveraging text data from social media and crowdsourcing.

# [Hartmut Geyer](https://www.cs.cmu.edu/~hgeyer/Research_Main.html)

My background is in **physics**. The workings of nature have always fascinated me. But I am most intrigued by how life exploits the laws of physics to solve tough engineering problems.

Dynamics and control of legged locomotion is such a tough engineering problem. The legged robots that we have built so far struggle with it; they barely survive outside the laboratory environment.  Nor do artificial legs reach far beyond peg legs that offer a mere support to their users. Even when legs are perfectly intact, as in spinal cord injured patients, we do not know how to engineer locomotion controls for them.

In my research, I try to understand the principles of legged dynamic systems underlying animal and human locomotion, how these principles impact on human motor control, and to apply this understanding to the design and control of legs in humanoid and rehabilitation robotics.

I believe that understanding the principles of legged dynamics and their connection to motor control will change how we simulate human locomotion, engineer artificial legs, and rehabilitate patients.

# [Jessica Hodgins](http://www.cs.cmu.edu/~jkh/)

  Jessica Hodgins is a Professor in the Robotics Institute and Computer Science Department at Carnegie Mellon University. From 2008-2016, she founded and ran research labs for Disney, rising to VP of Research and leading the labs in Pittsburgh and Los Angeles. From 2005-2015, she was Associate Director for Faculty in the Robotics Institute, running the promotion and tenure process and creating a mentoring program for pre-tenure faculty. Prior to moving to Carnegie Mellon in 2000, she was an Associate Professor and Assistant Dean in the College of Computing at Georgia Institute of Technology. She received her Ph.D. in Computer Science from Carnegie Mellon University in 1989. Her research focuses on computer graphics, animation, and robotics with an emphasis on generating and analyzing human motion. She has received a NSF Young Investigator Award, a Packard Fellowship, and a Sloan Fellowship. She was editor-in-chief of ACM Transactions on Graphics from 2000-2002 and ACM SIGGRAPH Papers Chair in 2003. She was an elected director at large on the ACM SIGGRAPH Executive Committee from 2012-2017 and in 2017 she was elected ACM SIGGRAPH President. In 2010, she was awarded the ACM SIGGRAPH Computer Graphics Achievement Award and in 2017 she was awarded the Steven Anson Coons Award for Outstanding Creative Contributions to Computer Graphics.

# [David Held](http://davheld.github.io/)

Young faculty

My research lies at the intersection of robotics, machine learning, and computer vision.

I am interested in developing methods for robotic perception and control that can allow robots to operate in in the messy, cluttered environments of our daily lives. My approach is to design new deep learning / machine learning algorithms to understand environmental changes: how dynamic objects in the environment can move and how to affect the environment to achieve a desired task.

I have applied this idea of learning to understand environmental changes to improve a robot's capabilities in two domains: **object manipulation** and autonomous driving. I am currently working on learning to control indoor robots for various object manipulation tasks, dealing with questions about multi-task learning, robust learning, simulation to real-world transfer, and safety. Within autonomous driving, I have shown how, by modeling object appearance changes, we can improve a robot's capabilities for every part of the robot perception pipeline: segmentation, tracking, velocity estimation, and object recognition. By teaching robots to understand and affect environmental changes, I hope to open the door to many new robotics applications, such as robots for our homes, assisted living facilities, schools, hospitals, or disaster relief areas.
The following video provides a decent overview of my current research and some of my current interests:
