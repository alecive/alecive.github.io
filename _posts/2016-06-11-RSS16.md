---
layout: post
title: RSS speech
description: My speech at RSS 16
category: hidden
tags: [RSS,speech]
article: yes
---

#### Slide 1 - [20 s]

 * Hello everybody, I am Alessandro Roncone, postdoc at the Yale Social Robotics Lab, and what I am going to present you today is a six degrees of freedom gaze controller for humanoid robots, joint work with Ugo Pattacini back when I was working at the Italian Institute of Technology.

#### Slide 2 - [52 s]

 * Our goal is the control of the head of the iCub Humanoid robot, which is equipped with a three degrees of freedom neck and a degrees of freedom eyes system with anthropomorphic arrangement, that is a binocular system capable of version and vergence behaviors. 

 * The applications are multiple: from the more standard tracking and attentive systems to image stabilization capabilities during whole body motions such as walking and balancing.
 
 * Importantly, an experiment made some years ago with the same gaze system I'm going to present here demonstrated that it is important to implement movements that are as human like as possible in the context of cooperative tasks with humans

 * To this end, let me show you some videos, that are far from depicting the state of the art but are a good representation of what we were trying to improve in terms of human-like movements.

#### VIDEO   - [30 s]

 * All of the systems shown here are characterized by exponential velocity profiles, with fast onset and slower decay toward the target.

 * Our goal has been to improve these types of trajectories and implement smooth, minimum jerk, bell shaped velocity profiles that have been found to be typical of a human movement.

#### Slide 3 - [30 s]

 * We defined the gaze problem as the task of controlling the head motors in order to move its fixation point toward a desired 3D position. 

 * As such, our task is inherently redundant, and for this reason, we have decided to implement a set of secondary behaviors to complement the main task, that is a vestibulo-ocular reflex, a gaze stabilization system, and saccadic behaviors

#### Slide 4 - [27 s]

 * And this is the architecture we implemented. Luckily for everyone, I don't have time to explain every single block shown here, by the main idea has been to provide a decoupled interface for the controller at the neck and the eyes and a two stage design for each of them, with an independent implementation of the inverse kinematics solver and the velocity controller.

 * The additional behaviors are targeting either one of the two subsystems and are added to the main goal if requested by the user.

#### VIDEO   - [95 s]

 * And this is a video that showcases the results. The robot is tracking the 3D location of the center of the red ball, and you can see how the implementation of minimum jerk trajectories improves over the standard gaze controller.

 * An important asset of our approach has been the design of the software interface that is exposed to the user, with flexibility in both the tuning of the internal parameters and the type of gaze target that can be asked for by the user.

 * And this one is instead a demonstration of the the saccadic behaviors, which are fast feedforward commands that exploit the faster dynamic of the eyes system in order to quickly foveate on the target. In our implementation, we are basically shortcutting the standard controller in order to find the configuration at the eyes that best converges onto the desired fixation point.

 * Finally, this is the stabilization system in action. The robot is able to both counteract for external disturbances by measuring the angular velocities at the IMU placed in the head, and at the same time to achieve the main goal of tracking the red ball.

 * This is instead a comparison of the optical flow at the left eye while performing predefined movements at the torso. It is clear how the image is more stable, and this is beneficial to any subsequent image processing software. 

#### Slide 6 - [46 s]

 * In conclusion, we have developed a comprehensive architecture for the explicit control of the fixation point of a generic binocular system. As a matter of fact, this software has been already ported to the Vizzy robot you can see in the picture.
 
 * Of course, the software has been released with an open source license and it is freely available online.

 * This ends my presentation. Thank you very much, and I am looking forward to see you at the poster session if you have questions about the details of the implementation!

